# azure-pipelines.yml
trigger:
- main

pool:
  vmImage: ubuntu-latest

stages:
- stage: RunTests
  displayName: 'Run API Tests'
  jobs:
  - job: TestJob
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.11'
        addToPath: true

    - script: |
        # create and use a virtual environment with the selected Python
        python -m venv .venv
        .venv/bin/python -m pip install --upgrade pip

        # Prefer pyproject.toml (Poetry) if present; fall back to requirements.txt
        if [ -f pyproject.toml ]; then
          .venv/bin/python -m pip install poetry
          .venv/bin/poetry export -f requirements.txt --without-hashes -o requirements-pip.txt || true
          if [ -f requirements-pip.txt ]; then
            .venv/bin/pip install -r requirements-pip.txt
          else
            .venv/bin/pip install "robotframework>=7.4.1,<8.0.0" "robotframework-requests>=0.9.7,<0.10.0"
          fi
        elif [ -f requirements.txt ]; then
          .venv/bin/pip install -r requirements.txt
        else
          .venv/bin/pip install "robotframework>=7.4.1,<8.0.0" "robotframework-requests>=0.9.7,<0.10.0"
        fi
      displayName: 'Create venv and install Robot Framework'

    - script: |
        # Diagnostic: show Python and key package versions installed in the venv
        echo "--- Python ---"
        .venv/bin/python -V || true
        echo "--- pip show ---"
        .venv/bin/pip show requests robotframework robotframework-requests || true
        echo "--- pip freeze ---"
        .venv/bin/pip freeze || true
      displayName: 'Show installed Python and package versions'

    - script: |
        # run tests using the venv python to ensure robot is available
        if [ -x .venv/bin/python ]; then
          .venv/bin/python -m robot.run -d results tests/api
        else
          echo "Virtual environment missing or python not executable"
          exit 1
        fi
      displayName: 'Run API robotfrmework'

    - task: PublishTestResults@2
      condition: succeededOrFailed()
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: 'results/output.xml'
       # searchFolder: '$(System.DefaultWorkingDirectory)/results'
       # mergeTestResults: true
        testRunTitle: 'Robot Framework Tests'

    - task: PublishBuildArtifacts@1
      condition: succeededOrFailed()
      inputs:
        PathtoPublish: '$(System.DefaultWorkingDirectory)/results'
        ArtifactName: 'robot-test-results'
        publishLocation: 'Container'

- stage: AnalyzeTests
  displayName: 'Analyze Test Results with AI'
  dependsOn: RunTests
  condition: succeededOrFailed()
  jobs:
  - job: AnalysisJob
    steps:
    - task: DownloadBuildArtifacts@0
      displayName: 'Download Test Results'
      inputs:
        buildType: 'current'
        downloadType: 'specific'
        itemPattern: 'robot-test-results/**'
        downloadPath: '$(System.ArtifactsDirectory)'
  
    - task: AzureCLI@2
      displayName: 'Call Azure AI API to Analyze Tests'
      inputs:
        azureSubscription: mi_ai_agent
        scriptType: bash
        scriptLocation: inlineScript
        inlineScript: |
          # Read test results
          TEST_RESULTS=$(cat $(System.ArtifactsDirectory)/robot-test-results/output.xml)
          
          # Build JSON payload using jq to properly escape all values
          PAYLOAD=$(jq -n \
            --arg results "$TEST_RESULTS" \
            '{
              "messages": [
                {
                  "role": "user",
                  "content": ("You are a QA test analyst. Analyze the Robot Framework test results below and provide a clear, professional report. Use this exact format:\n\n## TEST EXECUTION SUMMARY\n- Total Tests: [count]\n- Passed: [count]\n- Failed: [count]\n- Pass Rate: [percentage]\n\n## FAILED TESTS\nList each failed test with:\n- Test Name\n- Failure Reason (brief)\n- Error Details\n\n## ROOT CAUSES\nIdentify the main issues causing failures:\n1. [Issue 1]\n2. [Issue 2]\n(etc.)\n\n## SOLUTIONS\nProvide specific fixes for each issue:\n1. [Issue] -> Solution\n2. [Issue] -> Solution\n(etc.)\n\n## ACTION ITEMS\n- [ ] Fix item 1\n- [ ] Fix item 2\n(etc.)\n\nTest Results:\n" + $results)
                }
              ],
              "max_completion_tokens": 13107,
              "temperature": 0.5,
              "top_p": 1,
              "frequency_penalty": 0,
              "presence_penalty": 0,
              "model": "gpt-4.1-mini"
            }')
          
          # Send to AI for analysis
          ANALYSIS=$(curl -s -X POST "https://sonalaitest.cognitiveservices.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2025-01-01-preview" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer $(AZURE_API_KEY)" \
              -d "$PAYLOAD")
          
          # Extract the analysis text from the JSON response
          ANALYSIS_TEXT=$(echo "$ANALYSIS" | jq -r '.choices[0].message.content // .error.message // "Error: Could not parse response"')
          
          # Save both JSON and readable text
          mkdir -p $(System.ArtifactsDirectory)/ai-analysis
          echo "$ANALYSIS" > $(System.ArtifactsDirectory)/ai-analysis/test-analysis.json
          echo "$ANALYSIS_TEXT" > $(System.ArtifactsDirectory)/ai-analysis/test-analysis.txt
          
          # Display analysis in logs for easy viewing
          echo "================================"
          echo "TEST ANALYSIS RESULTS"
          echo "================================"
          echo "$ANALYSIS_TEXT"
          echo "================================"
      env:
        AZURE_API_KEY: $(AZURE_API_KEY)
    
    - task: PublishBuildArtifacts@1
      displayName: 'Publish AI Analysis'
      condition: succeededOrFailed()
      inputs:
        PathtoPublish: '$(System.ArtifactsDirectory)/ai-analysis'
        ArtifactName: 'ai-test-analysis'
        publishLocation: 'Container'