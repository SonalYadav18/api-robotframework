# azure-pipelines.yml
trigger:
- main

pool:
  name: local-agent # your self-hosted agent

stages:
- stage: RunTests
  displayName: 'Run API Tests'
  jobs:
  - job: TestJob
    steps:
    - script: |
        if command -v python3 >/dev/null 2>&1; then
          PY=python3
        elif command -v python >/dev/null 2>&1; then
          PY=python
        else
          echo "Python is not installed on the agent"
          exit 1
        fi
        $PY -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          $PY -m pip install -r requirements.txt
        else
          $PY -m pip install robotframework
        fi
      displayName: 'Install Robot Framework and dependencies'

    - script: robot -d results tests/api
      displayName: 'Run API robotfrmework'

    - task: PublishTestResults@2
      condition: succeededOrFailed()
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: 'results/output.xml'
       # searchFolder: '$(System.DefaultWorkingDirectory)/results'
       # mergeTestResults: true
        testRunTitle: 'Robot Framework Tests'

    - task: PublishBuildArtifacts@1
      condition: succeededOrFailed()
      inputs:
        PathtoPublish: '$(System.DefaultWorkingDirectory)/results'
        ArtifactName: 'robot-test-results'
        publishLocation: 'Container'

- stage: AnalyzeTests
  displayName: 'Analyze Test Results with AI'
  dependsOn: RunTests
  jobs:
  - job: AnalysisJob
    steps:
    - task: DownloadBuildArtifacts@0
      displayName: 'Download Test Results'
      inputs:
        buildType: 'current'
        downloadType: 'specific'
        itemPattern: 'robot-test-results/**'
        downloadPath: '$(System.ArtifactsDirectory)'
  
    - task: AzureCLI@2
      displayName: 'Call Azure AI API to Analyze Tests'
      inputs:
        azureSubscription: mi_ai_agent
        scriptType: bash
        scriptLocation: inlineScript
        inlineScript: |
          az --version
          az account show
          export AZURE_AI_AUTH_TOKEN=$(az account get-access-token --scope https://ai.azure.com/.default --query accessToken -o tsv)
          echo "Token saved to AZURE_AI_AUTH_TOKEN"
          
          # Read the test results
          TEST_RESULTS=$(cat $(System.ArtifactsDirectory)/robot-test-results/output.xml)
          
          # Send to AI for analysis
          ANALYSIS=$(curl -X POST "https://sonalaitest.cognitiveservices.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2025-01-01-preview" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer $(AZURE_API_KEY)" \
              -d "{
                  \"messages\": [
                      {
                          \"role\": \"user\",
                          \"content\": \"Please analyze the following Robot Framework test results and provide a comprehensive summary with insights. If any tests failed, identify the root causes and provide detailed solutions to fix them. Format the response with sections for: 1) Summary, 2) Failed Tests Analysis, 3) Recommended Solutions, 4) Action Items. Test Results: $TEST_RESULTS\"
                      }
                  ],
                  \"max_completion_tokens\": 13107,
                  \"temperature\": 1,
                  \"top_p\": 1,
                  \"frequency_penalty\": 0,
                  \"presence_penalty\": 0,
                  \"model\": \"gpt-4.1-mini\"
              }")
          
          # Save analysis to file
          mkdir -p $(System.ArtifactsDirectory)/ai-analysis
          echo "$ANALYSIS" > $(System.ArtifactsDirectory)/ai-analysis/test-analysis.json
          echo "Analysis saved to test-analysis.json"
      env:
        AZURE_API_KEY: $(AZURE_API_KEY)
    
    - task: PublishBuildArtifacts@1
      displayName: 'Publish AI Analysis'
      condition: succeededOrFailed()
      inputs:
        PathtoPublish: '$(System.ArtifactsDirectory)/ai-analysis'
        ArtifactName: 'ai-test-analysis'
        publishLocation: 'Container'